{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sathya Sravya  20161121\n",
    "#FINAL RESULTS OBTAINED \n",
    "\n",
    "#pca comps = 100 to 110\n",
    "#lda comps = 350\n",
    "#num of trees = 460\n",
    "#max depth = 420 to 450\n",
    "#min tree split = 14 \n",
    "\n",
    "\n",
    "#number of neurons in first layer = max possible 5000\n",
    "#num of pca comps = 100\n",
    "#activation functions order : relu logistic tanh identity\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NOTE \n",
    "\n",
    "Here functions with main suffix run that respective classifier on three data types and functions with prefixes vary can be commented and run .You can call those functions if you want to vary those parameters else comment them out in the last cell (26th) (heading : caller to all functions)\n",
    "\n",
    "And the function names are self explanatory !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pdb\n",
    "import requests\n",
    "from collections import defaultdict\n",
    "import random \n",
    "import time\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn import tree\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "#import snips as snp\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "\n",
    "from tqdm import *\n",
    "\n",
    "from functools import wraps\n",
    "from time import time as _timenow \n",
    "from sys import stderr\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load CIFAR-10 Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_cifar():\n",
    "    \n",
    "    trn_data, trn_labels, tst_data, tst_labels = [], [], [], []\n",
    "    def unpickle(file):\n",
    "        with open(file, 'rb') as fo:\n",
    "            data = pickle.load(fo, encoding='latin1')\n",
    "        return data\n",
    "    \n",
    "    for i in trange(1):\n",
    "        batchName = './data/data_batch_{0}'.format(i + 1)\n",
    "        unpickled = unpickle(batchName)\n",
    "        trn_data.extend(unpickled['data'])\n",
    "        trn_labels.extend(unpickled['labels'])\n",
    "    unpickled = unpickle('./data/test_batch')\n",
    "    tst_data.extend(unpickled['data'])\n",
    "    tst_labels.extend(unpickled['labels'])\n",
    "    return trn_data, trn_labels, tst_data, tst_labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_prep(train_data,test_data):\n",
    "    ''' pre-processes the given image\n",
    "        performs mean normalization and other such operations http://scikit-learn.org/stable/modules/preprocessing.html'''\n",
    "    #feature scaling\n",
    "    scaler = preprocessing.StandardScaler().fit(train_data)\n",
    "    train_processed = scaler.transform(train_data)\n",
    "    test_processed = scaler.transform(test_data)\n",
    "    \n",
    "    return train_processed,test_processed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensionality reduction using PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_dim(**kwargs):\n",
    "    ''' performs dimensionality reduction\n",
    "    https://stackabuse.com/implementing-lda-in-python-with-scikit-learn/'''\n",
    "    if kwargs['method'] == 'pca':\n",
    "        pca = PCA(n_components=kwargs['num_comp'])\n",
    "        pca.whiten = True\n",
    "        pca = pca.fit(kwargs['train_data'])\n",
    "        train_reduced = pca.transform(kwargs['train_data'])\n",
    "        test_reduced = pca.transform(kwargs['test_data'])\n",
    "        return train_reduced,test_reduced\n",
    "    if kwargs['method'] == 'lda':\n",
    "        lda = LDA(n_components=kwargs['num_comp'])\n",
    "        lda = lda.fit(kwargs['train_data'],kwargs['train_label'])\n",
    "        train_reduced = lda.transform(kwargs['train_data'])\n",
    "        test_reduced = lda.transform(kwargs['test_data'])\n",
    "        return train_reduced,test_reduced\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification using decision tree,SVM(rbf),MLP and LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(X, y, **kwargs):\n",
    "    ''' trains a classifier by taking input features\n",
    "        and their respective targets and returns the trained model'''\n",
    "    if kwargs['method'] == 'CART':\n",
    "        clf = RandomForestClassifier(n_estimators=kwargs['num_trees'], min_samples_split=kwargs['min_samples_split'], max_depth =kwargs['max_depth_tree'],bootstrap=kwargs['bootstr'])\n",
    "        clf.fit(X,y)\n",
    "        return clf\n",
    "    if kwargs['method'] == 'SVM':\n",
    "        clf_svm = SVC(kernel=kwargs['choice_kernel'],\n",
    "                    max_iter=kwargs['num_iter'],C=kwargs['c'],\n",
    "                    degree=kwargs['degree'])\n",
    "        clf_svm.fit(X,y)\n",
    "        return clf_svm\n",
    "    if kwargs['method']=='MLP':\n",
    "        mlp = MLPClassifier(hidden_layer_sizes=(kwargs['h1']\n",
    "                                                , ),activation= kwargs['activationfn']                  \n",
    "                    )#,activation , max_iter=200 default (early_stopping boolfalse default)solver=kwargs['sgd'],\n",
    "        #mlp.activation = \n",
    "        mlp.fit(X, y)\n",
    "        #snp.prettyplot(matplotlib)\n",
    "        #fig, ax = snp.newfig()\n",
    "        #ax.plot(classifier.loss_curve_)\n",
    "        #snp.labs(\"number of steps\", \"loss function\", \"Loss During GD (Rate=0.001)\")\n",
    "       # print(\"Training set score: %f\" % mlp.score(X, y))\n",
    "        return mlp\n",
    "    if kwargs['method']=='LogisticRegression':\n",
    "        lr = LogisticRegression(penalty=kwargs['penalty'],\n",
    "                    max_iter=kwargs['num_iter'],C=kwargs['c'])\n",
    "        lr.fit(X,y)\n",
    "        return lr\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(target, predicted):\n",
    "    f1 = f1_score(target, predicted, average='micro')\n",
    "    acc = accuracy_score(target, predicted)\n",
    "    return f1, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(**kwargs):\n",
    "    '''takes test data and trained classifier model,\n",
    "    performs classification and prints accuracy and f1-score'''\n",
    "    if(kwargs['method']=='CART'):\n",
    "        model = kwargs['model']\n",
    "        X_test = kwargs['test']\n",
    "        predicted = model.predict(X_test)     \n",
    "        return predicted\n",
    "    if(kwargs['method']=='MLP'):\n",
    "        model = kwargs['model']\n",
    "        y_test = kwargs['test_label']\n",
    "        X_test = kwargs['test']\n",
    "        predicted = model.predict(X_test)\n",
    "        print(\"Test set score: %f\" % model.score(X_test, y_test))\n",
    "        return predicted\n",
    "    else:\n",
    "        model = kwargs['model']\n",
    "        y_test = kwargs['test_label']\n",
    "        X_test = kwargs['test']\n",
    "        predicted = model.predict(X_test)\n",
    "        return predicted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function 'main' for all tree classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_tree():\n",
    "    trn_data, trn_labels, tst_data, tst_labels = load_cifar()\n",
    "    #trn_data, tst_data = image_prep(trn_data, tst_data)#, list(map(image_prep, tst_data))\n",
    "    X_train, X_val, y_train, y_val = train_test_split(trn_data, trn_labels,test_size = 0.20) \n",
    "    ''' perform dimesioality reduction/feature extraction and classify the features into one of 10 classses\n",
    "        print accuracy and f1-score.\n",
    "    https://www.geeksforgeeks.org/args-kwargs-python/ #kwargs usage\n",
    "        '''\n",
    "    trprep_data, teprep_data = image_prep(X_train, X_val) \n",
    "        \n",
    "    tr_data1,te_data1 = reduce_dim(train_data=trprep_data,test_data=teprep_data,train_label=y_train,method='pca',num_comp=100)\n",
    "    tr_data2,te_data2 = reduce_dim(train_data=trprep_data,test_data=teprep_data,train_label=y_train,method='lda',num_comp=350)\n",
    "    \n",
    "    model = classify(tr_data1, y_train, num_trees=600, min_samples_split=4, max_depth_tree=470,bootstr =True,method='CART')\n",
    "    modelmlp = classify(tr_data1, y_train,method='MLP',\n",
    "                        h1=5000,activationfn='logistic')\n",
    "    modellr = classify(tr_data1,y_train,method='LogisticRegression',penalty='l2',num_iter=150,c=1.0)\n",
    "    \n",
    "    labels_predicted = test(test=te_data1,test_label=y_val,model=model,method='CART')\n",
    "    #labels_predicted = test(test=te_data1,test_label=tst_labels,model=modellr,method='LogisticRegression')\n",
    "    f_score, accuracy = evaluate(y_val, labels_predicted)\n",
    "    print('Val-PCA reduced data - F1 score: {}\\n Accuracy: {}'.format(f_score, accuracy))\n",
    "    \n",
    "    model = classify(trprep_data, y_train, num_trees=600, min_samples_split=4, max_depth_tree=470,bootstr =True,method='CART')\n",
    "    labels_predicted = test(test=teprep_data,test_label=y_val,model=model,method='CART')\n",
    "    #labels_predicted = test(test=te_data1,test_label=tst_labels,model=modellr,method='LogisticRegression')\n",
    "    f_score, accuracy = evaluate(y_val, labels_predicted)\n",
    "    print('Val-RAW data - F1 score: {}\\n Accuracy: {}'.format(f_score, accuracy))\n",
    "    \n",
    "    model = classify(tr_data2, y_train, num_trees=600, min_samples_split=4, max_depth_tree=470,bootstr =True,method='CART')\n",
    "    labels_predicted = test(test=te_data2,test_label=y_val,model=model,method='CART')\n",
    "    #labels_predicted = test(test=te_data1,test_label=tst_labels,model=modellr,method='LogisticRegression')\n",
    "    f_score, accuracy = evaluate(y_val, labels_predicted)\n",
    "    print('Val-LDA reduced data - F1 score: {}\\n Accuracy: {}'.format(f_score, accuracy))\n",
    "    \n",
    "    \n",
    "    \n",
    "#main_tree()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter tuning for Decision trees "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vary_num_trees():#trprep_data,teprep_data,y_train,y_test): \n",
    "    num_trees_arr=[]\n",
    "    acc=[]\n",
    "    i=200\n",
    "    trn_data, trn_labels, tst_data, tst_labels = load_cifar()\n",
    "   # trn_data, tst_data = image_prep(trn_data, tst_data)#, list(map(image_prep, tst_data))\n",
    "    X_train, X_val, y_train, y_val = train_test_split(trn_data, trn_labels,test_size = 0.20) \n",
    "    ''' perform dimesioality reduction/feature extraction and classify the features into one of 10 classses\n",
    "        print accuracy and f1-score.\n",
    "    https://www.geeksforgeeks.org/args-kwargs-python/ #kwargs usage\n",
    "        '''\n",
    "    trprep_data, teprep_data = image_prep(X_train, X_val)\n",
    "    trprep_data,teprep_data = reduce_dim(train_data=trprep_data,test_data=teprep_data,train_label=y_train,method='pca',num_comp=100)\n",
    "    while(i<620):\n",
    "        #train_data, test_data = image_prep(X_train, X_test)\n",
    "        model = classify(trprep_data, y_train, num_trees=i, min_samples_split=4,max_depth_tree=470,bootstr =True, method='CART')\n",
    "        output = test(test = teprep_data,test_label=y_val, model= model, method='CART')\n",
    "        f_score, accuracy = evaluate(y_val, output)\n",
    "        print('Val - F1 score: {}\\n Accuracy: {}'.format(f_score, accuracy))\n",
    "        acc.append(accuracy)\n",
    "        num_trees_arr.append(i)\n",
    "        \n",
    "        i+=50\n",
    "        \n",
    "    plt.plot(num_trees_arr, acc)\n",
    "    plt.xlabel('No. of trees used') \n",
    "    plt.ylabel('Accuracy') \n",
    "    plt.show()\n",
    "#vary_num_trees()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vary_num_pca(): #using randomforestclassifer\n",
    "    num_comp_pca=[]\n",
    "    acc=[]\n",
    "    i=50\n",
    "    trn_data, trn_labels, tst_data, tst_labels = load_cifar()\n",
    "   # trn_data, tst_data = image_prep(trn_data, tst_data)#, list(map(image_prep, tst_data))\n",
    "    X_train, X_val, y_train, y_val = train_test_split(trn_data, trn_labels,test_size = 0.20) \n",
    "    trprep_data, teprep_data = image_prep(X_train, X_val)\n",
    "    while(i<360):\n",
    "        tr_data1,te_data1 = reduce_dim(train_data=trprep_data,test_data=teprep_data,train_label=y_train,method='pca',num_comp=i)\n",
    "        model = classify(tr_data1, y_train, num_trees=600, min_samples_split=4,max_depth_tree=470,bootstr =True, method='CART')\n",
    "        output = test(test = te_data1, test_label=y_val,model= model, method='CART')\n",
    "        f_score, accuracy = evaluate(y_val, output)\n",
    "        print('Val - F1 score: {}\\n Accuracy: {}'.format(f_score, accuracy))\n",
    "        acc.append(accuracy)\n",
    "        num_comp_pca.append(i)\n",
    "        i+=50\n",
    "    \n",
    "    plt.plot(num_comp_pca, acc)\n",
    "    plt.xlabel('No. of pca components used') \n",
    "    plt.ylabel('Accuracy') \n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vary_num_lda(): #using randomforestclassifer\n",
    "    num_comp_lda=[]\n",
    "    acc=[]\n",
    "    i=50\n",
    "    trn_data, trn_labels, tst_data, tst_labels = load_cifar()\n",
    "   # trn_data, tst_data = image_prep(trn_data, tst_data)#, list(map(image_prep, tst_data))\n",
    "    X_train, X_val, y_train, y_val = train_test_split(trn_data, trn_labels,test_size = 0.20) \n",
    "    trprep_data, teprep_data = image_prep(X_train, X_val)\n",
    "    while(i<800):\n",
    "        tr_data1,te_data1 = reduce_dim(train_data=trprep_data,test_data=teprep_data,train_label=y_train,method='lda',num_comp=i)\n",
    "        model = classify(tr_data1, y_train, num_trees=600, min_samples_split=4,max_depth_tree=470,bootstr =True, method='CART')\n",
    "        output = test(test = te_data1, model= model, method='CART')\n",
    "        f_score, accuracy = evaluate(y_val, output)\n",
    "        print('Val - F1 score: {}\\n Accuracy: {}'.format(f_score, accuracy))\n",
    "        acc.append(accuracy)\n",
    "        num_comp_lda.append(i)\n",
    "        i+=50\n",
    "    \n",
    "    plt.plot(num_comp_lda, acc)\n",
    "    plt.xlabel('No. of LDA components used') \n",
    "    plt.ylabel('Accuracy') \n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vary_min_split_tree():\n",
    "    min_split_arr=[]\n",
    "    acc=[]\n",
    "    i=2\n",
    "    trn_data, trn_labels, tst_data, tst_labels = load_cifar()\n",
    "    #trn_data, tst_data = image_prep(trn_data, tst_data)#, list(map(image_prep, tst_data))\n",
    "    X_train, X_val, y_train, y_val = train_test_split(trn_data, trn_labels,test_size = 0.20) \n",
    "    trprep_data, teprep_data = image_prep(X_train, X_val)\n",
    "    trprep_data,teprep_data = reduce_dim(train_data=trprep_data,test_data=teprep_data,train_label=y_train,method='pca',num_comp=100)\n",
    "    while(i<17):\n",
    "        model = classify(trprep_data, y_train, num_trees=570, min_samples_split=i,max_depth_tree=455,bootstr =True, method='CART')\n",
    "        output = test(test = teprep_data, model= model, method='CART')\n",
    "        f_score, accuracy = evaluate(y_val, output)\n",
    "        print('Val - F1 score: {}\\n Accuracy: {}'.format(f_score, accuracy))\n",
    "        acc.append(accuracy)\n",
    "        min_split_arr.append(i)\n",
    "        \n",
    "        i+=2\n",
    "        \n",
    "    plt.plot(min_split_arr, acc)\n",
    "    plt.xlabel('No. of splits minimum used') \n",
    "    plt.ylabel('Accuracy') \n",
    "    plt.show()\n",
    "    \n",
    "#vary_min_split_tree()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vary_depth_tree():\n",
    "    max_depth_arr=[]\n",
    "    acc=[]\n",
    "    i=50\n",
    "    trn_data, trn_labels, tst_data, tst_labels = load_cifar()\n",
    "    #trn_data, tst_data = image_prep(trn_data, tst_data)#, list(map(image_prep, tst_data))\n",
    "    X_train, X_val, y_train, y_val = train_test_split(trn_data, trn_labels,test_size = 0.20) \n",
    "    trprep_data, teprep_data = image_prep(X_train, X_val)\n",
    "    trprep_data,teprep_data = reduce_dim(train_data=trprep_data,test_data=teprep_data,train_label=y_train,method='pca',num_comp=100)\n",
    "    while(i<990):\n",
    "        model = classify(trprep_data, y_train, num_trees=300, min_samples_split=5,max_depth_tree=i,bootstr =True, method='CART')\n",
    "        output = test(test = teprep_data, model= model, method='CART')\n",
    "        f_score, accuracy = evaluate(y_val, output)\n",
    "        print('Val - F1 score: {}\\n Accuracy: {}'.format(f_score, accuracy))\n",
    "        acc.append(accuracy)\n",
    "        max_depth_arr.append(i)\n",
    "        \n",
    "        i+=100\n",
    "        \n",
    "    plt.plot(max_depth_arr, acc)\n",
    "    plt.xlabel('Max depth that can be used') \n",
    "    plt.ylabel('Accuracy') \n",
    "    plt.show()\n",
    "    \n",
    "#vary_depth_tree()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM (rbf kernel) main call and tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_svm():\n",
    "    trn_data, trn_labels, tst_data, tst_labels = load_cifar()\n",
    "    X_train, X_val, y_train, y_val = train_test_split(trn_data, trn_labels,test_size = 0.20) \n",
    "    ''' perform dimesioality reduction/feature extraction and classify the features into one of 10 classses\n",
    "        print accuracy and f1-score.\n",
    "    https://www.geeksforgeeks.org/args-kwargs-python/ #kwargs usage\n",
    "        '''\n",
    "    trprep_data, teprep_data = image_prep(X_train, X_val) \n",
    "        \n",
    "    tr_data1,te_data1 = reduce_dim(train_data=trprep_data,test_data=teprep_data,train_label=y_train,method='pca',num_comp=100)\n",
    "    tr_data2,te_data2 = reduce_dim(train_data=trprep_data,test_data=teprep_data,train_label=y_train,method='lda',num_comp=350)\n",
    "    modelsvmrbf = classify(tr_data1,y_train,method='SVM',choice_kernel='rbf',num_iter=150,c=1.0,degree=3)\n",
    "    \n",
    "    labels_predicted = test(test=te_data1,test_label=y_val,model=modelsvmrbf,method='SVM')\n",
    "    #labels_predicted = test(test=te_data1,test_label=tst_labels,model=modellr,method='LogisticRegression')\n",
    "    f_score, accuracy = evaluate(y_val, labels_predicted)\n",
    "    print('Val-PCA reduced data -SVM - F1 score: {}\\n Accuracy: {}'.format(f_score, accuracy))\n",
    "    \n",
    "    modelsvmrbf = classify(trprep_data,y_train,method='SVM',choice_kernel='rbf',num_iter=150,c=1.0,degree=3)\n",
    "    \n",
    "    labels_predicted = test(test=teprep_data,test_label=y_val,model=modelsvmrbf,method='SVM')\n",
    "    #labels_predicted = test(test=te_data1,test_label=tst_labels,model=modellr,method='LogisticRegression')\n",
    "    f_score, accuracy = evaluate(y_val, labels_predicted)\n",
    "    print('Val-RAW data -SVM - F1 score: {}\\n Accuracy: {}'.format(f_score, accuracy))\n",
    "    \n",
    "    modelsvmrbf = classify(tr_data2,y_train,method='SVM',choice_kernel='rbf',num_iter=150,c=1.0,degree=3)\n",
    "    \n",
    "    labels_predicted = test(test=te_data2,test_label=y_val,model=modelsvmrbf,method='SVM')\n",
    "    #labels_predicted = test(test=te_data1,test_label=tst_labels,model=modellr,method='LogisticRegression')\n",
    "    f_score, accuracy = evaluate(y_val, labels_predicted)\n",
    "    print('Val-LDA reduced data -SVM - F1 score: {}\\n Accuracy: {}'.format(f_score, accuracy))\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vary_ker_svm():\n",
    "    trn_data, trn_labels, tst_data, tst_labels = load_cifar()\n",
    "    X_train, X_val, y_train, y_val = train_test_split(trn_data, trn_labels,test_size = 0.20) \n",
    "    ''' perform dimesioality reduction/feature extraction and classify the features into one of 10 classses\n",
    "        print accuracy and f1-score.\n",
    "    https://www.geeksforgeeks.org/args-kwargs-python/ #kwargs usage\n",
    "        '''\n",
    "    trprep_data, teprep_data = image_prep(X_train, X_val) \n",
    "    str = ['linear','poly','rbf','sigmoid']    \n",
    "    res_arr=[]\n",
    "    acc = []\n",
    "    tr_data1,te_data1 = reduce_dim(train_data=trprep_data,test_data=teprep_data,train_label=y_train,method='pca',num_comp=100)\n",
    "    i =0\n",
    "    while(i<4):\n",
    "    \n",
    "        modelsvm = classify(tr_data1,y_train,method='SVM',choice_kernel=str[i],num_iter=150,c=1.0,degree=3)\n",
    "    \n",
    "        labels_predicted = test(test=te_data1,test_label=y_val,model=modelsvm,method='SVM')\n",
    "        #labels_predicted = test(test=te_data1,test_label=tst_labels,model=modellr,method='LogisticRegression')\n",
    "        f_score, accuracy = evaluate(y_val, labels_predicted)\n",
    "        print('Val-PCA reduced data -SVM - F1 score: {}\\n Accuracy: {}'.format(f_score, accuracy))\n",
    "        acc.append(accuracy)\n",
    "        res_arr.append(i)\n",
    "        \n",
    "        i+=1\n",
    "        \n",
    "        \n",
    "    plt.plot(res_arr, acc)\n",
    "    plt.xlabel('Max depth that can be used') \n",
    "    plt.ylabel('Accuracy') \n",
    "    plt.show()\n",
    "    \n",
    "#vary_ker_svm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vary_num_iter_svm():\n",
    "    trn_data, trn_labels, tst_data, tst_labels = load_cifar()\n",
    "    X_train, X_val, y_train, y_val = train_test_split(trn_data, trn_labels,test_size = 0.20) \n",
    "    ''' perform dimesioality reduction/feature extraction and classify the features into one of 10 classses\n",
    "        print accuracy and f1-score.\n",
    "    https://www.geeksforgeeks.org/args-kwargs-python/ #kwargs usage\n",
    "        '''\n",
    "    trprep_data, teprep_data = image_prep(X_train, X_val) \n",
    "    res_arr=[]\n",
    "    acc = []\n",
    "    tr_data1,te_data1 = reduce_dim(train_data=trprep_data,test_data=teprep_data,train_label=y_train,method='pca',num_comp=100)\n",
    "    i =150\n",
    "    while(i<6200):\n",
    "    \n",
    "        modelsvm = classify(tr_data1,y_train,method='SVM',choice_kernel='rbf',num_iter=i,c=1.0,degree=3)\n",
    "    \n",
    "        labels_predicted = test(test=te_data1,test_label=y_val,model=modelsvm,method='SVM')\n",
    "        #labels_predicted = test(test=te_data1,test_label=tst_labels,model=modellr,method='LogisticRegression')\n",
    "        f_score, accuracy = evaluate(y_val, labels_predicted)\n",
    "        print('Val-PCA reduced data -SVM - F1 score: {}\\n Accuracy: {}'.format(f_score, accuracy))\n",
    "        acc.append(accuracy)\n",
    "        res_arr.append(i)\n",
    "        \n",
    "        i+=1000\n",
    "        \n",
    "        \n",
    "    plt.plot(res_arr, acc)\n",
    "    plt.xlabel('Max number of iterations that can be used') \n",
    "    plt.ylabel('Accuracy') \n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vary_pca_svm():\n",
    "    trn_data, trn_labels, tst_data, tst_labels = load_cifar()\n",
    "    X_train, X_val, y_train, y_val = train_test_split(trn_data, trn_labels,test_size = 0.20) \n",
    "    ''' perform dimesioality reduction/feature extraction and classify the features into one of 10 classses\n",
    "        print accuracy and f1-score.\n",
    "    https://www.geeksforgeeks.org/args-kwargs-python/ #kwargs usage\n",
    "        '''\n",
    "    trprep_data, teprep_data = image_prep(X_train, X_val) \n",
    "    res_arr=[]\n",
    "    acc = []\n",
    "    \n",
    "    i =150\n",
    "    while(i<1200):\n",
    "        tr_data1,te_data1 = reduce_dim(train_data=trprep_data,test_data=teprep_data,train_label=y_train,method='pca',num_comp=i)\n",
    "        modelsvm = classify(tr_data1,y_train,method='SVM',choice_kernel='rbf',num_iter=1200,c=1.0,degree=3)\n",
    "    \n",
    "        labels_predicted = test(test=te_data1,test_label=y_val,model=modelsvm,method='SVM')\n",
    "        #labels_predicted = test(test=te_data1,test_label=tst_labels,model=modellr,method='LogisticRegression')\n",
    "        f_score, accuracy = evaluate(y_val, labels_predicted)\n",
    "        print('Val-PCA reduced data -SVM - F1 score: {}\\n Accuracy: {}'.format(f_score, accuracy))\n",
    "        acc.append(accuracy)\n",
    "        res_arr.append(i)\n",
    "        \n",
    "        i+=100\n",
    "        \n",
    "        \n",
    "    plt.plot(res_arr, acc)\n",
    "    plt.xlabel('Max number pca components that can be used') \n",
    "    plt.ylabel('Accuracy') \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main call for classifying 3 data types and Parameter tuning For MLP "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_mlp():\n",
    "    trn_data, trn_labels, tst_data, tst_labels = load_cifar()\n",
    "    #trn_data, tst_data = image_prep(trn_data, tst_data)#, list(map(image_prep, tst_data))\n",
    "    X_train, X_val, y_train, y_val = train_test_split(trn_data, trn_labels,test_size = 0.20) \n",
    "    ''' perform dimesioality reduction/feature extraction and classify the features into one of 10 classses\n",
    "        print accuracy and f1-score.\n",
    "    https://www.geeksforgeeks.org/args-kwargs-python/ #kwargs usage\n",
    "        '''\n",
    "    trprep_data, teprep_data = image_prep(X_train, X_val) \n",
    "        \n",
    "    tr_data1,te_data1 = reduce_dim(train_data=trprep_data,test_data=teprep_data,train_label=y_train,method='pca',num_comp=100)\n",
    "    tr_data2,te_data2 = reduce_dim(train_data=trprep_data,test_data=teprep_data,train_label=y_train,method='lda',num_comp=350)\n",
    "    \n",
    "    modelmlp = classify(tr_data1, y_train,method='MLP',\n",
    "                        h1=5000,activationfn='relu')\n",
    "    labels_predicted = test(test=te_data1,test_label=y_val,model=modelmlp,method='MLP')\n",
    "    #labels_predicted = test(test=te_data1,test_label=tst_labels,model=modellr,method='LogisticRegression')\n",
    "    f_score, accuracy = evaluate(y_val, labels_predicted)\n",
    "    print('Val-PCA reduced data MLP - F1 score: {}\\n Accuracy: {}'.format(f_score, accuracy))\n",
    "    \n",
    "    modelmlp = classify(trprep_data, y_train,method='MLP',\n",
    "                        h1=5000,activationfn='relu')\n",
    "    labels_predicted = test(test=teprep_data,test_label=y_val,model=modelmlp,method='MLP')\n",
    "    #labels_predicted = test(test=te_data1,test_label=tst_labels,model=modellr,method='LogisticRegression')\n",
    "    f_score, accuracy = evaluate(y_val, labels_predicted)\n",
    "    print('Val-RAW data MLP - F1 score: {}\\n Accuracy: {}'.format(f_score, accuracy))\n",
    "    \n",
    "    modelmlp = classify(tr_data2, y_train,method='MLP',\n",
    "                        h1=5000,activationfn='relu')\n",
    "    labels_predicted = test(test=te_data2,test_label=y_val,model=modelmlp,method='MLP')\n",
    "    #labels_predicted = test(test=te_data1,test_label=tst_labels,model=modellr,method='LogisticRegression')\n",
    "    f_score, accuracy = evaluate(y_val, labels_predicted)\n",
    "    print('Val-LDA reduced data MLP - F1 score: {}\\n Accuracy: {}'.format(f_score, accuracy))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_neurons_l1():\n",
    "    num_neurons_l1_arr=[]\n",
    "    acc=[]\n",
    "    i=3000\n",
    "    trn_data, trn_labels, tst_data, tst_labels = load_cifar()\n",
    "    #trn_data, tst_data = image_prep(trn_data, tst_data)#, list(map(image_prep, tst_data))\n",
    "    X_train, X_val, y_train, y_val = train_test_split(trn_data, trn_labels,test_size = 0.20) \n",
    "    trprep_data, teprep_data = image_prep(X_train, X_val)\n",
    "    tr_data1,te_data1 = reduce_dim(train_data=trprep_data,test_data=teprep_data,train_label=y_train,method='pca',num_comp=100)\n",
    "    while(i<5002):\n",
    "        modelmlp = classify(tr_data1, y_train,method='MLP',\n",
    "                        h1=i,activationfn='logistic')\n",
    "    \n",
    "    \n",
    "        labels_predicted = test(test=te_data1,test_label=y_val,model=modelmlp,method='MLP')\n",
    "        f_score, accuracy = evaluate(y_val, labels_predicted)\n",
    "     \n",
    "    \n",
    "        print('Val - F1 score: {}\\n Accuracy: {}'.format(f_score, accuracy))\n",
    "        acc.append(accuracy)\n",
    "        num_neurons_l1_arr.append(i)\n",
    "        \n",
    "        i+=1000\n",
    "        \n",
    "    plt.plot(num_neurons_l1_arr, acc)\n",
    "    plt.xlabel('Max neurons that can be used') \n",
    "    plt.ylabel('Accuracy') \n",
    "    plt.show()\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vary_activ_fn():\n",
    "    str=['logistic','relu','tanh','identity']\n",
    "    actfn_index_arr=[]\n",
    "    acc=[]\n",
    "    i=0\n",
    "    trn_data, trn_labels, tst_data, tst_labels = load_cifar()\n",
    "    #trn_data, tst_data = image_prep(trn_data, tst_data)#, list(map(image_prep, tst_data))\n",
    "    X_train, X_val, y_train, y_val = train_test_split(trn_data, trn_labels,test_size = 0.20) \n",
    "    trprep_data, teprep_data = image_prep(X_train, X_val)\n",
    "    tr_data1,te_data1 = reduce_dim(train_data=trprep_data,test_data=teprep_data,train_label=y_train,method='pca',num_comp=100)\n",
    "    while(i<4):\n",
    "        modelmlp = classify(tr_data1, y_train,method='MLP',\n",
    "                        h1=4000,activationfn=str[i])\n",
    "    \n",
    "    \n",
    "        labels_predicted = test(test=te_data1,test_label=y_val,model=modelmlp,method='MLP')\n",
    "        f_score, accuracy = evaluate(y_val, labels_predicted)\n",
    "     \n",
    "    \n",
    "        print('Val - F1 score: {}\\n Accuracy: {}'.format(f_score, accuracy))\n",
    "        acc.append(accuracy)\n",
    "        actfn_index_arr.append(i)     \n",
    "        \n",
    "        \n",
    "        i+=1\n",
    "        \n",
    "    plt.plot(actfn_index_arr, acc)\n",
    "    plt.xlabel('Different activation functions that are used') \n",
    "    plt.ylabel('Accuracy') \n",
    "    plt.show()\n",
    "    print(modelmlp.n_iter_)\n",
    "    print(modelmlp.n_layers_)\n",
    "    print(modelmlp.n_outputs_)\n",
    "    print(modelmlp.out_activation_) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vary_num_pca_mlp(): #using mlp\n",
    "    num_comp_pca=[]\n",
    "    acc=[]\n",
    "    i=50\n",
    "    trn_data, trn_labels, tst_data, tst_labels = load_cifar()\n",
    "   # trn_data, tst_data = image_prep(trn_data, tst_data)#, list(map(image_prep, tst_data))\n",
    "    X_train, X_val, y_train, y_val = train_test_split(trn_data, trn_labels,test_size = 0.20) \n",
    "    trprep_data, teprep_data = image_prep(X_train, X_val)\n",
    "    while(i<360):\n",
    "        tr_data1,te_data1 = reduce_dim(train_data=trprep_data,test_data=teprep_data,train_label=y_train,method='pca',num_comp=i)\n",
    "        modelmlp = classify(tr_data1, y_train,method='MLP',\n",
    "                        h1=4000,activationfn='relu')\n",
    "    \n",
    "    \n",
    "        labels_predicted = test(test=te_data1,test_label=y_val,model=modelmlp,method='MLP')\n",
    "        f_score, accuracy = evaluate(y_val, labels_predicted)\n",
    "     \n",
    "        print('Val - F1 score: {}\\n Accuracy: {}'.format(f_score, accuracy))\n",
    "        acc.append(accuracy)\n",
    "        num_comp_pca.append(i)\n",
    "        i+=50\n",
    "    \n",
    "    plt.plot(num_comp_pca, acc)\n",
    "    plt.xlabel('No. of pca components used in MLP') \n",
    "    plt.ylabel('Accuracy') \n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main call for classifying 3 data types and Parameter tuning for Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_lr():\n",
    "    trn_data, trn_labels, tst_data, tst_labels = load_cifar()\n",
    "    #trn_data, tst_data = image_prep(trn_data, tst_data)#, list(map(image_prep, tst_data))\n",
    "    X_train, X_val, y_train, y_val = train_test_split(trn_data, trn_labels,test_size = 0.20) \n",
    "    ''' perform dimesioality reduction/feature extraction and classify the features into one of 10 classses\n",
    "        print accuracy and f1-score.\n",
    "    https://www.geeksforgeeks.org/args-kwargs-python/ #kwargs usage\n",
    "        '''\n",
    "    trprep_data, teprep_data = image_prep(X_train, X_val) \n",
    "        \n",
    "    tr_data1,te_data1 = reduce_dim(train_data=trprep_data,test_data=teprep_data,train_label=y_train,method='pca',num_comp=100)\n",
    "    tr_data2,te_data2 = reduce_dim(train_data=trprep_data,test_data=teprep_data,train_label=y_train,method='lda',num_comp=350)\n",
    "    modellr = classify(trprep_data,y_train,method='LogisticRegression',penalty='l2',num_iter=150,c=1.0)\n",
    "      \n",
    "    labels_predicted = test(test=teprep_data,test_label=y_val,model=modellr,method='LogisticRegression')\n",
    "    f_score, accuracy = evaluate(y_val, labels_predicted)\n",
    "    print('Val - F1 score: {}\\n Accuracy: {}'.format(f_score, accuracy))\n",
    "    \n",
    "    modellr = classify(tr_data1,y_train,method='LogisticRegression',penalty='l2',num_iter=150,c=1.0)\n",
    "      \n",
    "    labels_predicted = test(test=te_data1,test_label=y_val,model=modellr,method='LogisticRegression')\n",
    "    f_score, accuracy = evaluate(y_val, labels_predicted)\n",
    "    print('Val - F1 score: {}\\n Accuracy: {}'.format(f_score, accuracy))\n",
    "    \n",
    "    modellr = classify(tr_data2,y_train,test_label=y_val,method='LogisticRegression',penalty='l2',num_iter=150,c=1.0)\n",
    "      \n",
    "    labels_predicted = test(test=te_data2,test_label=y_val,model=modellr,method='LogisticRegression')\n",
    "    f_score, accuracy = evaluate(y_val, labels_predicted)\n",
    "    print('Val - F1 score: {}\\n Accuracy: {}'.format(f_score, accuracy))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vary_c_lr():\n",
    "    c_val_arr=[]\n",
    "    acc=[]\n",
    "    i=1.0\n",
    "    trn_data, trn_labels, tst_data, tst_labels = load_cifar()\n",
    "    \n",
    "    X_train, X_val, y_train, y_val = train_test_split(trn_data, trn_labels,test_size = 0.20) \n",
    "    trprep_data, teprep_data = image_prep(X_train, X_val)\n",
    "    #trprep_data,teprep_data = reduce_dim(train_data=trprep_data,test_data=teprep_data,train_label=y_train,method='pca',num_comp=100)\n",
    "    while(i<15):\n",
    "        modellr = classify(trprep_data,y_train,method='LogisticRegression',penalty='l2',num_iter=150,c=i)\n",
    "      \n",
    "        labels_predicted = test(test=teprep_data,test_label=y_val,model=modellr,method='LogisticRegression')\n",
    "        f_score, accuracy = evaluate(y_val, labels_predicted)\n",
    "        print('Val - F1 score: {}\\n Accuracy: {}'.format(f_score, accuracy))\n",
    "        acc.append(accuracy)\n",
    "        c_val_arr.append(i) \n",
    "        i+=1\n",
    "    plt.plot(c_val_arr, acc)\n",
    "    plt.xlabel('C values (LR) ')\n",
    "    plt.show()\n",
    "#vary_c_lr()      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vary_num_iter_lr():\n",
    "    iter_val_arr=[]\n",
    "    acc=[]\n",
    "    i=150\n",
    "    trn_data, trn_labels, tst_data, tst_labels = load_cifar()\n",
    "    \n",
    "    X_train, X_val, y_train, y_val = train_test_split(trn_data, trn_labels,test_size = 0.20) \n",
    "    trprep_data, teprep_data = image_prep(X_train, X_val)\n",
    "    trprep_data,teprep_data = reduce_dim(train_data=trprep_data,test_data=teprep_data,train_label=y_train,method='pca',num_comp=100)\n",
    "    while(i<5159):\n",
    "        modellr = classify(trprep_data,y_train,method='LogisticRegression',penalty='l2',num_iter=i,c=1.0)\n",
    "      \n",
    "        labels_predicted = test(test=teprep_data,test_label=y_val,model=modellr,method='LogisticRegression')\n",
    "        f_score, accuracy = evaluate(y_val, labels_predicted)\n",
    "        print('Val - F1 score: {}\\n Accuracy: {}'.format(f_score, accuracy))\n",
    "        acc.append(accuracy)\n",
    "        iter_val_arr.append(i) \n",
    "        i+=500\n",
    "    plt.plot(iter_val_arr, acc)\n",
    "    plt.xlabel('Number of iterationsin LR')\n",
    "    plt.show()\n",
    "#vary_num_iter_lr()       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Caller of all functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (<ipython-input-26-018f62993263>, line 27)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-26-018f62993263>\"\u001b[0;36m, line \u001b[0;32m27\u001b[0m\n\u001b[0;31m    \u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    \n",
    "    \n",
    "   \n",
    "    #vary_num_trees()\n",
    "    #vary_min_split_tree()\n",
    "    #vary_depth_tree()\n",
    "    #vary_num_pca()\n",
    "    #vary_num_lda()\n",
    "    \n",
    "    #vary_num_pca_mlp()\n",
    "    #num_neurons_l1()\n",
    "    #vary_activ_fn()\n",
    "    \n",
    "    #main_lr()\n",
    "    #vary_c_lr()\n",
    "    #vary_num_iter_lr()\n",
    "    \n",
    "    #vary_pca_svm()\n",
    "    #vary_num_iter_svm()\n",
    "    #vary_ker_svm()\n",
    "    \n",
    "    #main_svm()\n",
    "    \n",
    "    #main_tree()\n",
    "    #main_mlp()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
